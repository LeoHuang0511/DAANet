{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# from  config import cfg\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "from misc.utils import *\n",
    "# from model.VIC import Video_Individual_Counter\n",
    "# from model.video_crowd_count import video_crowd_count\n",
    "from model.video_people_flux import DutyMOFANet\n",
    "from model.points_from_den import get_ROI_and_MatchInfo\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import matplotlib.cm as cm\n",
    "from train import compute_metrics_single_scene,compute_metrics_all_scenes\n",
    "import  os.path as osp\n",
    "from misc.gt_generate import *\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='VIC test and demo',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\n",
    "    '--DATASET', type=str, default='SENSE',\n",
    "    help='Directory where to write output frames (If None, no output)')\n",
    "parser.add_argument(\n",
    "    '--TASK', type=str, default='FT',\n",
    "    help='Directory where to write output frames (If None, no output)')\n",
    "parser.add_argument(\n",
    "    '--OUTPUT_DIR', type=str, default='./visualization',\n",
    "    help='Directory where to write output frames (If None, no output)')\n",
    "parser.add_argument(\n",
    "    '--TEST_INTERVALS', type=int, default=11,\n",
    "    help='Directory where to write output frames (If None, no output)')\n",
    "parser.add_argument(\n",
    "    '--SKIP_FLAG', type=bool, default=True,\n",
    "    help='if you need to caculate the MIAE and MOAE, it should be False')\n",
    "parser.add_argument(\n",
    "    '--SAVE_FREQ', type=int, default=200,\n",
    "    help='Directory where to write output frames (If None, no output)')\n",
    "parser.add_argument(\n",
    "    '--SEED', type=int, default=3035,\n",
    "    help='Directory where to write output frames (If None, no output)')\n",
    "parser.add_argument(\n",
    "    '--GPU_ID', type=str, default='1',\n",
    "    help='Directory where to write output frames (If None, no output)')\n",
    "\n",
    "parser.add_argument('--VAL_BATCH_SIZE', type=int, default=1)\n",
    "\n",
    "\n",
    "parser.add_argument('--TRAIN_SIZE', type=int, nargs='+', default=[768,1024])\n",
    "parser.add_argument('--FEATURE_SCALE', type=float, default=1/4.)\n",
    "\n",
    "\n",
    "parser.add_argument('--DEN_FACTOR', type=float, default=200.)\n",
    "parser.add_argument('--MEAN_STD', type=tuple, default=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "parser.add_argument('--ROI_RADIUS', type=float, default=4.)\n",
    "parser.add_argument('--GAUSSIAN_SIGMA', type=float, default=4)\n",
    "parser.add_argument('--CONF_BLOCK_SIZE', type=int, default=16)\n",
    "\n",
    "parser.add_argument('--BACKBONE', type=str, default='vgg')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "    '--MODEL_PATH', type=str, default='',\n",
    "    help='pretrained weight path')\n",
    "\n",
    "# parser.add_argument(\n",
    "#     '--MODEL_PATH', type=str, default='./exp/SENSE/03-22_17-33_SENSE_VGG16_FPN_5e-05/ep_15_iter_115000_mae_2.gaussian_kernel1_mse_3.677_seq_MAE_6.439_WRAE_9.506_MIAE_1.447_MOAE_1.474.pth',\n",
    "#     help='pretrained weight path')\n",
    "\n",
    "\n",
    "opt = parser.parse_known_args()[0]\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "\n",
    "opt.VAL_INTERVALS = opt.TEST_INTERVALS\n",
    "\n",
    "opt.MODE = 'vis'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def Target(root,i,frame):\n",
    "    img_ids = os.listdir(root)\n",
    "    img_ids.sort()\n",
    "    labels=[]\n",
    "    gts = defaultdict(list)\n",
    "    with open(root.replace('video_ori', 'label_list_all')+'.txt', 'r') as f: #label_list_all_rmInvalid\n",
    "        lines = f.readlines()\n",
    "        for lin in lines:\n",
    "            lin_list = [i for i in lin.rstrip().split(' ')]\n",
    "            ind = lin_list[0]\n",
    "            lin_list = [float(i) for i in lin_list[3:] if i != '']\n",
    "            assert len(lin_list) % 7 == 0\n",
    "            gts[ind] = lin_list\n",
    "        \n",
    "    img_id = frame.strip()\n",
    "    # single_path = osp.path.join(root, img_id)\n",
    "    label = gts[img_id]\n",
    "    box_and_point = torch.tensor(label).view(-1, 7).contiguous()\n",
    "\n",
    "    points = box_and_point[:, 4:6].float()\n",
    "    ids = (box_and_point[:, 6]).long()\n",
    "\n",
    "    if ids.size(0)>0:\n",
    "        sigma = 0.6*torch.stack([(box_and_point[:,2]-box_and_point[:,0])/2,(box_and_point[:,3]-box_and_point[:,1])/2],1).min(1)[0]  #torch.sqrt(((box_and_point[:,2]-box_and_point[:,0])/2)**2 + ((box_and_point[:,3]-box_and_point[:,1])/2)**2)\n",
    "    else:\n",
    "        sigma = torch.tensor([])\n",
    "\n",
    "    labels.append({'scene_name':i,'frame':int(img_id.split('.')[0].replace('_resize','')), 'person_id':ids, 'points':points, 'sigma':sigma})\n",
    "\n",
    "    return labels\n",
    "\n",
    "# def Target(base_path,i,frame):\n",
    "#     # img_path = []\n",
    "#     labels=[]\n",
    "#     root  = osp.join(base_path,'img1')\n",
    "#     img_ids = os.listdir(root)\n",
    "#     img_ids.sort()\n",
    "#     gts = defaultdict(list)\n",
    "#     with open(osp.join(root.replace('img1', 'gt'), 'gt.txt'), 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         for lin in lines:\n",
    "#             lin_list = [float(i) for i in lin.rstrip().split(',')]\n",
    "#             ind = int(lin_list[0])\n",
    "#             gts[ind].append(lin_list)\n",
    "\n",
    "#     img_id = frame.strip()\n",
    "#     # single_path = osp.join(root, img_id)\n",
    "#     annotation  = gts[int(img_id.split('.')[0].replace('_resize',''))]\n",
    "#     annotation = torch.tensor(annotation,dtype=torch.float32)\n",
    "#     box = annotation[:,2:6]\n",
    "#     points =   box[:,0:2] + box[:,2:4]/2\n",
    "\n",
    "#     sigma = torch.min(box[:,2:4], 1)[0] / 2.\n",
    "#     ids = annotation[:,1].long()\n",
    "#     # img_path.append(single_path)\n",
    "\n",
    "#     labels.append({'scene_name':i,'frame':int(img_id.split('.')[0].replace('_resize','')), 'person_id':ids, 'points':points,'sigma':sigma})\n",
    "    # return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nfs/home/leo0511/Research/datasets/SENSE/video_ori/video_001/img1/000012.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m target2 \u001b[38;5;241m=\u001b[39m Target(img_2_path,scene_name,img2_frame\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# img1 = Image.open(os.path.join(img_1_path,img1_frame+'.jpg'))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# img2 = Image.open(os.path.join(img_2_path,img2_frame+'.jpg'))\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg1/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mimg1_frame\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m img2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(img_2_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg1/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mimg2_frame\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img1\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Research/research_env/lib/python3.8/site-packages/PIL/Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3128\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3131\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3132\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nfs/home/leo0511/Research/datasets/SENSE/video_ori/video_001/img1/000012.jpg'"
     ]
    }
   ],
   "source": [
    "# opt.MODEL_PATH = \"/nfs/home/leo0511/Research/DutyMOFA/exp/CARLA/best/carla_ep_14_iter_22000_mae_7.288_mse_8.240_seq_MAE_24.390_seq_MSE_24.390_WRAE_10.697_MIAE_2.812_MOAE_2.347.pth\"\n",
    "# opt.MODEL_PATH = \"/nfs/home/leo0511/Research/DutyMOFA/exp/HT21/02-21_14-25_vgg__HT21_5e-05_0.0001/latest_state.pth\"\n",
    "opt.MODEL_PATH = \"/nfs/home/leo0511/Research/DutyMOFA/exp/SENSE/02-27_14-01_vgg_vis_SENSE_5e-05_0.0001/latest_state.pth\"\n",
    "dataset = \"SENSE\"\n",
    "\n",
    "\n",
    "# scene_name = \"02\" #\"1019_IMG_1639_cut_01\"\n",
    "# img1_frame = '000343'\n",
    "# img2_frame = '000395'\n",
    "\n",
    "# scene_name = \"HT21-02\" #\"1019_IMG_1639_cut_01\"\n",
    "# img1_frame = '000336'\n",
    "# img2_frame = '000390'\n",
    "\n",
    "scene_name = \"v_00005\" #\"1019_IMG_1639_cut_01\"\n",
    "img1_frame = '00000001'\n",
    "img2_frame = '00000001'\n",
    "\n",
    "# img_1_path = f\"/nfs/home/leo0511/Research/datasets/{dataset}/train/{scene_name}\"\n",
    "# img_2_path = f\"/nfs/home/leo0511/Research/datasets/{dataset}/train/{scene_name}\"\n",
    "\n",
    "img_1_path = f\"/nfs/home/leo0511/Research/datasets/SENSE/video_ori/{scene_name}\"\n",
    "img_2_path = f\"/nfs/home/leo0511/Research/datasets/SENSE/video_ori/{scene_name}\"\n",
    "\n",
    "target1 = Target(img_1_path,scene_name,img1_frame+'.jpg')\n",
    "target2 = Target(img_2_path,scene_name,img2_frame+'.jpg')\n",
    "img1 = Image.open(os.path.join(img_1_path,img1_frame+'.jpg'))\n",
    "img2 = Image.open(os.path.join(img_2_path,img2_frame+'.jpg'))\n",
    "# img1 = Image.open(os.path.join(img_1_path,'img1/'+img1_frame+'.jpg'))\n",
    "# img2 = Image.open(os.path.join(img_2_path,'img1/'+img2_frame+'.jpg'))\n",
    "if img1.mode != 'RGB':\n",
    "    img1=img1.convert('RGB')\n",
    "if img2.mode != 'RGB':\n",
    "    img2 = img2.convert('RGB')\n",
    "\n",
    "\n",
    "img_transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(*opt.MEAN_STD)\n",
    "    ])\n",
    "\n",
    "img1 = img_transform(img1)\n",
    "img2 = img_transform(img2)\n",
    "img = [[img1,img2]]\n",
    "target = [target1[0],target2[0]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path:  /nfs/home/leo0511/Research/DutyMOFA/exp/HT21/02-21_14-25_vgg__HT21_5e-05_0.0001/latest_state.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/leo0511/Research/research_env/lib/python3.8/site-packages/torch/nn/functional.py:4084: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "# from config import cfg\n",
    "from importlib import import_module\n",
    "import misc.transforms as own_transforms\n",
    "\n",
    "\n",
    "    # ------------prepare enviroment------------\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.GPU_ID\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ------------prepare data loader------------\n",
    "data_mode = opt.DATASET\n",
    "datasetting = import_module(f'datasets.setting.{data_mode}')\n",
    "cfg_data = datasetting.cfg_data\n",
    "\n",
    "# ------------Start Training------------\n",
    "cfg=opt\n",
    "print(\"model_path: \",cfg.MODEL_PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net = DutyMOFANet(cfg, cfg_data)\n",
    "    \n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda:\"+str(torch.cuda.current_device()))\n",
    "    \n",
    "\n",
    "    state_dict = torch.load(cfg.MODEL_PATH,map_location=device)\n",
    "    \n",
    "    try:\n",
    "        net.load_state_dict(state_dict['net'], strict=True)\n",
    "    except:\n",
    "        net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "    net.cuda()\n",
    "    net.eval()\n",
    "\n",
    "    generate_gt = GenerateGT(cfg)\n",
    "    get_roi_and_matchinfo = get_ROI_and_MatchInfo( cfg.TRAIN_SIZE, cfg.ROI_RADIUS, feature_scale=cfg.FEATURE_SCALE)\n",
    "\n",
    "\n",
    "  \n",
    "    img,target = img[0],target\n",
    "    # scene_name = target[0]['scene_name']\n",
    "    img = torch.stack(img, 0).cuda()\n",
    "    b, c, h, w = img.shape\n",
    "    if h % 64 != 0:\n",
    "        pad_h = 64 - h % 64\n",
    "    else:\n",
    "        pad_h = 0\n",
    "    if w % 64 != 0:\n",
    "        pad_w = 64 - w % 64\n",
    "    else:\n",
    "        pad_w = 0\n",
    "    pad_dims = (0, pad_w, 0, pad_h)\n",
    "    img = F.pad(img, pad_dims, \"constant\")\n",
    "    img_pair_num = img.shape[0]//2\n",
    "\n",
    "    \n",
    "\n",
    "    den_scales, pred_map, mask, out_den, in_den, den_prob, io_prob, confidence, f_flow, b_flow, feature1, feature2, attn_1, attn_2 = net(img)\n",
    "\n",
    "    pre_inflow, pre_outflow = \\\n",
    "        in_den.sum().detach().cpu(), out_den.sum().detach().cpu()\n",
    "    \n",
    "    target_ratio = pred_map.shape[2]/img.shape[2]\n",
    "\n",
    "    for b in range(len(target)):\n",
    "       \n",
    "        \n",
    "        for key,data in target[b].items():\n",
    "            if torch.is_tensor(data):\n",
    "                target[b][key]=data.cuda()\n",
    "    #    -----------gt generate metric computation------------------\n",
    "        \n",
    "    gt_den_scales = generate_gt.get_den(den_scales[0].shape, target, target_ratio, scale_num=len(den_scales))\n",
    "    gt_den = gt_den_scales[0]\n",
    "    \n",
    "    assert pred_map.size() == gt_den.size()\n",
    "\n",
    "    gt_io_map = torch.zeros(img_pair_num, 2, den_scales[0].size(2), den_scales[0].size(3)).cuda()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    gt_in_cnt = torch.zeros(img_pair_num).detach()\n",
    "    gt_out_cnt = torch.zeros(img_pair_num).detach()\n",
    "\n",
    "    assert pred_map.size() == gt_den.size()\n",
    "\n",
    "    for pair_idx in range(img_pair_num):\n",
    "        count_in_pair=[target[pair_idx * 2]['points'].size(0), target[pair_idx * 2+1]['points'].size(0)]\n",
    "        \n",
    "        if (np.array(count_in_pair) > 0).all() and (np.array(count_in_pair) < 4000).all():\n",
    "            match_gt, _ = get_roi_and_matchinfo(target[pair_idx * 2], target[pair_idx * 2+1],'ab')\n",
    "\n",
    "            gt_io_map, gt_in_cnt, gt_out_cnt \\\n",
    "                = generate_gt.get_pair_io_map(pair_idx, target, match_gt, gt_io_map, gt_out_cnt, gt_in_cnt, target_ratio)\n",
    "                \n",
    "                # = generate_gt.get_pair_seg_map(pair_idx, target, match_gt, gt_io_map, gt_out_cnt, gt_in_cnt, target_ratio)\n",
    "    gt_mask = (gt_io_map>0).float()\n",
    "    restore_transform =standard_transforms.Compose([\n",
    "        own_transforms.DeNormalize(*cfg.MEAN_STD),\n",
    "        standard_transforms.ToPILImage()\n",
    "    ])\n",
    "\n",
    "\n",
    "    # save_results_mask(cfg, None, None, scene_name, (img1_frame, vi+cfg.TEST_INTERVALS), restore_transform, 0, \n",
    "    #                     img[0].clone().unsqueeze(0), img[1].clone().unsqueeze(0),\\\n",
    "    #                     pred_map[0].detach().cpu().numpy(), pred_map[1].detach().cpu().numpy(),out_den[0].detach().cpu().numpy(), in_den[0].detach().cpu().numpy(), gt_io_map[0].unsqueeze(0).detach().cpu().numpy(),\\\n",
    "    #                     (confidence[0,:,:,:]).unsqueeze(0).detach().cpu().numpy(),(confidence[1,:,:,:]).unsqueeze(0).detach().cpu().numpy(),\\\n",
    "    #                     f_flow , b_flow, [attn_1,attn_1,attn_1], [attn_2,attn_2,attn_2], den_scales, gt_den_scales, \\\n",
    "    #                     [mask,mask,mask], [gt_mask,gt_mask,gt_mask], [den_prob,den_prob,den_prob], [io_prob,io_prob,io_prob])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_count_0 221\n",
      "count_0 214.10000000000002\n",
      "count_0_scale_0 65.05\n",
      "count_0_scale_1 115.12\n",
      "count_0_scale_2 33.93\n",
      "total: 214.10000000000002\n",
      "gt_count_1 219\n",
      "count_1 211.95\n",
      "count_1_scale_0 64.07\n",
      "count_1_scale_1 114.72\n",
      "count_1_scale_2 33.16\n",
      "total: 211.95\n",
      "gt_in: 17\n",
      "gt_out: 19\n",
      "in: 18.03\n",
      "out: 22.18\n"
     ]
    }
   ],
   "source": [
    "gt_count_0 = len(target[0]['person_id'])\n",
    "gt_count_1 = len(target[1]['person_id'])\n",
    "\n",
    "count_0_scale = []\n",
    "count_1_scale = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    count_0_scale.append(round(torch.sum(den_scales[i][0]).item(),2))\n",
    "    count_1_scale.append(round(torch.sum(den_scales[i][1]).item(),2))\n",
    "# count_0 = round(torch.sum(pred_map[0]).item(),2)\n",
    "# count_1 = round(torch.sum(pred_map[1]).item(),2)\n",
    "count_0 = sum(count_0_scale)\n",
    "count_1 = sum(count_1_scale)\n",
    "\n",
    "gt_in = len(match_gt['un_b'])\n",
    "gt_out = len(match_gt['un_a'])\n",
    "count_in = round(torch.sum(in_den).item(),2)\n",
    "count_out = round(torch.sum(out_den).item(),2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "print('gt_count_0',gt_count_0)\n",
    "print('count_0',count_0)\n",
    "for i in range(3):\n",
    "    print(f\"count_0_scale_{i}\", count_0_scale[i])\n",
    "print(\"total:\",sum(count_0_scale))\n",
    "\n",
    "print('gt_count_1',gt_count_1)\n",
    "print('count_1',count_1)\n",
    "for i in range(3):\n",
    "    print(f\"count_1_scale_{i}\", count_1_scale[i])\n",
    "print(\"total:\",sum(count_1_scale))\n",
    "\n",
    "print(\"gt_in:\",(gt_in))\n",
    "print(\"gt_out:\",(gt_out))\n",
    "print(\"in:\",(count_in))\n",
    "print(\"out:\",(count_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "restore = restore_transform\n",
    "batch = 0 \n",
    "img0 = img[0].clone().unsqueeze(0)[0]\n",
    "img1 = img[1].clone().unsqueeze(0)[0]\n",
    "den0 =  pred_map[0].detach().cpu().numpy()[0]\n",
    "den1 = pred_map[1].detach().cpu().numpy()[0]\n",
    "out_map = out_den[0].detach().cpu().numpy()[0]\n",
    "# in_map = in_den[0].detach().cpu().numpy()[0]\n",
    "# gt_io_map = gt_io_map[0].unsqueeze(0).detach().cpu().numpy()[0]\n",
    "conf0=(confidence[0,:,:,:]).unsqueeze(0).detach().cpu().numpy()[0]\n",
    "conf1=(confidence[1,:,:,:]).unsqueeze(0).detach().cpu().numpy()[0]\n",
    "gt_den_scales=gt_den_scales\n",
    "mask=[mask,mask,mask]\n",
    "gt_mask=[gt_mask,gt_mask,gt_mask]\n",
    "den_probs=[den_prob,den_prob,den_prob]\n",
    "io_probs= [io_prob,io_prob,io_prob]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_kernel = 31\n",
    "gaussian_sigma = 10\n",
    "\n",
    "\n",
    "cfg.TRAIN_BATCH_SIZE = 1\n",
    "\n",
    "pil_to_tensor = standard_transforms.ToTensor()\n",
    "\n",
    "UNIT_H , UNIT_W = img0.size(1), img0.size(2)\n",
    "# for idx, tensor in enumerate(zip(img0.cpu().data, img1.cpu().data,pred_map0, gt_map0, pred_map1, gt_map1, \\\n",
    "#                                  pred_mask_out, gt_mask_out, pred_mask_in, gt_mask_in, attn_1, attn_2)):\n",
    "\n",
    "if cfg.MODE == 'test':\n",
    "    cfg.TRAIN_BATCH_SIZE = cfg.VAL_BATCH_SIZE\n",
    "\n",
    "COLOR_MAP = [\n",
    "    [255, 0, 0],\n",
    "    [0, 0, 255],\n",
    "    [0, 255, 255],\n",
    "]\n",
    "COLOR_MAP = np.array(COLOR_MAP, dtype=\"uint8\")\n",
    "COLOR_MAP_CONF = [\n",
    "    [255, 255, 0],\n",
    "    [255, 0, 255],\n",
    "    [0, 255, 255],\n",
    "]\n",
    "COLOR_MAP_CONF = np.array(COLOR_MAP_CONF, dtype=\"uint8\")\n",
    "\n",
    "\n",
    "\n",
    "den_scales_1_map = []\n",
    "gt_den_scales_1_map = []\n",
    "den_scales_2_map = []\n",
    "gt_den_scales_2_map = []\n",
    "mask_in_scales_1_map = []\n",
    "gt_mask_in_scales_1_map = []\n",
    "mask_out_scales_1_map = []\n",
    "gt_mask_out_scales_1_map = []\n",
    "# den_prob_map_1 = []\n",
    "# io_prob_map_1 = []\n",
    "# den_prob_map_2 = []\n",
    "# io_prob_map_2 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pil_input0 = restore(img0.cpu().data)\n",
    "pil_input1 = restore(img1.cpu().data)\n",
    "\n",
    "\n",
    "a = [0,0,0]\n",
    "\n",
    "\n",
    "for i in range(len(den_scales)):\n",
    "\n",
    "    den_scale_1 = den_scales[i][0].detach().cpu().numpy()[0]\n",
    "    den_scale_2 = den_scales[i][1].detach().cpu().numpy()[0]\n",
    "    gt_den_scale_1 = gt_den_scales[i][0].detach().cpu().numpy()[0]\n",
    "    gt_den_scale_2 = gt_den_scales[i][1].detach().cpu().numpy()[0]\n",
    "    den_scale_1 = cv2.GaussianBlur(den_scale_1, (int(gaussian_kernel/2**i+a[i]),int(gaussian_kernel/2**i+a[i]),),int(10/2**i))\n",
    "    den_scale_2 = cv2.GaussianBlur(den_scale_2, (int(gaussian_kernel/2**i+a[i]),int(gaussian_kernel/2**i+a[i]),),int(10/2**i))\n",
    "    gt_den_scale_1 = cv2.GaussianBlur(gt_den_scale_1, (int(gaussian_kernel/2**i+a[i]),int(gaussian_kernel/2**i+a[i]),),int(10/2**i))\n",
    "    gt_den_scale_2 = cv2.GaussianBlur(gt_den_scale_2, (int(gaussian_kernel/2**i+a[i]),int(gaussian_kernel/2**i+a[i]),),int(10/2**i))\n",
    "\n",
    "\n",
    "\n",
    "    den_scale_1 = cv2.resize(cv2.applyColorMap((255 * den_scale_1 / (den_scale_1.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "    den_scale_2 = cv2.resize(cv2.applyColorMap((255 * den_scale_2 / (den_scale_2.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "    gt_den_scale_1 = cv2.resize(cv2.applyColorMap((255 * gt_den_scale_1 / (gt_den_scale_1.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "    gt_den_scale_2 = cv2.resize(cv2.applyColorMap((255 * gt_den_scale_2 / (gt_den_scale_2.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "    # den_scale_1 = cv2.cvtColor(den_scale_1, cv2.COLOR_BGR2RGB)\n",
    "    # den_scale_2 = cv2.cvtColor(den_scale_2, cv2.COLOR_BGR2RGB)\n",
    "    # gt_den_scale_1 = cv2.cvtColor(gt_den_scale_1, cv2.COLOR_BGR2RGB)\n",
    "    # gt_den_scale_2 = cv2.cvtColor(gt_den_scale_2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    den_scales_1_map.append(den_scale_1)\n",
    "    den_scales_2_map.append(den_scale_2)\n",
    "    gt_den_scales_1_map.append(gt_den_scale_1)\n",
    "    gt_den_scales_2_map.append(gt_den_scale_2)\n",
    "\n",
    "\n",
    "    ########## mask ###############\n",
    "    mask_out_scale_1 = mask[i][0,:,:,:].detach().cpu().numpy()[0]\n",
    "    mask_out_scale_1[np.where(mask_out_scale_1<0.2)] = 0\n",
    "    mask_in_scale_1 =  mask[i][cfg.TRAIN_BATCH_SIZE,:,:,:].detach().cpu().numpy()[0]\n",
    "    \n",
    "    mask_out_scale_1 = cv2.GaussianBlur(mask_out_scale_1, (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "    mask_in_scale_1 = cv2.GaussianBlur(mask_in_scale_1, (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "    # mask_out_scale_1[mask_out_scale_1>0.2] = 1\n",
    "    # mask_in_scale_1[mask_in_scale_1>0.2] = 1\n",
    "\n",
    "    \n",
    "    gt_mask_out_scale_1 = gt_mask[i][0,0:1,:,:].detach().cpu().numpy()[0]\n",
    "    gt_mask_in_scale_1 = gt_mask[i][0,1:2,:,:].detach().cpu().numpy()[0]\n",
    "    \n",
    "    gt_mask_out_scale_1 = cv2.GaussianBlur(gt_mask_out_scale_1, (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "    gt_mask_in_scale_1 = cv2.GaussianBlur(gt_mask_in_scale_1, (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "    gt_mask_out_scale_1[gt_mask_out_scale_1>0.15] = 1\n",
    "    gt_mask_in_scale_1[gt_mask_in_scale_1>0.15] = 1\n",
    "\n",
    "    \n",
    "    # mask_out_scale_1 = np.clip((cv2.add(10*mask_out_scale_1,gaussian_sigma)), 0 , 255)\n",
    "    # mask_in_scale_1 = np.clip((cv2.add(10*mask_in_scale_1,gaussian_sigma)), 0 , 255)\n",
    "    # mask_out_scale_1 = (mask_out_scale_1-mask_out_scale_1.min())/(mask_out_scale_1.max()-mask_out_scale_1.min())\n",
    "    # mask_in_scale_1 = (mask_in_scale_1-mask_in_scale_1.min())/(mask_in_scale_1.max()-mask_in_scale_1.min())\n",
    "\n",
    "\n",
    "    mask_out_scale_1 = cv2.resize(cv2.applyColorMap((255 * mask_out_scale_1 / (mask_out_scale_1.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_HOT), (UNIT_W, UNIT_H)) \n",
    "    mask_in_scale_1 = cv2.resize(cv2.applyColorMap((255 * mask_in_scale_1 / (mask_in_scale_1.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_HOT), (UNIT_W, UNIT_H)) \n",
    "    gt_mask_out_scale_1 = cv2.resize(cv2.applyColorMap((255 * gt_mask_out_scale_1 / (gt_mask_out_scale_1.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_HOT), (UNIT_W, UNIT_H)) \n",
    "    gt_mask_in_scale_1 = cv2.resize(cv2.applyColorMap((255 * gt_mask_in_scale_1 / (gt_mask_in_scale_1.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_HOT), (UNIT_W, UNIT_H)) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # mask_out_scale_1 = cv2.cvtColor(mask_out_scale_1, cv2.COLOR_BGR2RGB)\n",
    "    # mask_in_scale_1 = cv2.cvtColor(mask_in_scale_1, cv2.COLOR_BGR2RGB)\n",
    "    # gt_mask_out_scale_1 = cv2.cvtColor(gt_mask_out_scale_1, cv2.COLOR_BGR2RGB)\n",
    "    # gt_mask_in_scale_1 = cv2.cvtColor(gt_mask_in_scale_1, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "\n",
    "    mask_out_scales_1_map.append(mask_out_scale_1)\n",
    "    mask_in_scales_1_map.append(mask_in_scale_1)\n",
    "    gt_mask_out_scales_1_map.append(gt_mask_out_scale_1)\n",
    "    gt_mask_in_scales_1_map.append(gt_mask_in_scale_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    den_prob_1 = den_probs[i][0,:,:,:].detach().cpu().numpy()\n",
    "    den_prob_2 =  den_probs[i][cfg.TRAIN_BATCH_SIZE,:,:,:].detach().cpu().numpy()\n",
    "\n",
    "    io_prob_1 = io_probs[i][0,:,:,:].detach().cpu().numpy()\n",
    "    io_prob_2 =  io_probs[i][cfg.TRAIN_BATCH_SIZE,:,:,:].detach().cpu().numpy()\n",
    "\n",
    "    conf0[i] = cv2.GaussianBlur(conf0[i], (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "    conf1[i] = cv2.GaussianBlur(conf1[i], (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ratio = UNIT_H/den0.shape[0]\n",
    "den0_map = cv2.GaussianBlur(den0, (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "den0_map = cv2.resize(cv2.applyColorMap((255 * den0_map / (den0_map.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "den1_map = cv2.GaussianBlur(den1, (gaussian_kernel,gaussian_kernel,),gaussian_sigma)\n",
    "den1_map = cv2.resize(cv2.applyColorMap((255 * den1_map / (den1_map.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "\n",
    "# out_map = cv2.resize(cv2.applyColorMap((255 * out_map / (out_map.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "# in_map = cv2.resize(cv2.applyColorMap((255 * in_map / (in_map.max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "\n",
    "# gt_out_map = cv2.resize(cv2.applyColorMap((255 * gt_io_map[0] / (gt_io_map[0].max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "# gt_in_map = cv2.resize(cv2.applyColorMap((255 * gt_io_map[1] / (gt_io_map[1].max() + 1e-10)).astype(np.uint8).squeeze(), cv2.COLORMAP_JET), (UNIT_W, UNIT_H)) \n",
    "\n",
    "\n",
    "def drawPoint(img, points):\n",
    "    points = np.ceil(points.cpu().numpy())\n",
    "    point_size = 1\n",
    "    point_color = (0,255,0)\n",
    "    thickness = 26\n",
    "    for point in points:\n",
    "        cv2.circle(img, [int(point[0]),int(point[1])], point_size, point_color, thickness)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "dot_map0 = np.full((conf0.shape[1],conf0.shape[2],3), 255).astype(np.uint8)\n",
    "dot_map0 = drawPoint(dot_map0, target[0][\"points\"])\n",
    "conf_map0 = np.argmax(conf0, axis=0)\n",
    "conf_map0 = cv2.resize(COLOR_MAP_CONF[conf_map0].squeeze(),  (UNIT_W, UNIT_H))\n",
    "conf_map0_dot = 255 - conf_map0 * np.repeat(((dot_map0[:,:,0])<255).squeeze(),3,axis=1).reshape(UNIT_H, UNIT_W, 3)\n",
    "\n",
    "\n",
    "dot_map1 = np.full((conf1.shape[1],conf1.shape[2],3), 255).astype(np.uint8)\n",
    "dot_map1 = drawPoint(dot_map1, target[1][\"points\"])\n",
    "conf_map1 = np.argmax(conf1, axis=0)\n",
    "conf_map1 = cv2.resize(COLOR_MAP_CONF[conf_map1].squeeze(),  (UNIT_W, UNIT_H))\n",
    "conf_map1_dot = 255 - conf_map1 * np.repeat((((dot_map1[:,:,0])<255)).squeeze(),3,axis=1).reshape(UNIT_H, UNIT_W, 3)\n",
    "\n",
    "conf_0_scale_0 = cv2.resize(cv2.applyColorMap((255 *  conf0[0] / ( conf0[0].max() + 1e-10)).astype(np.uint8), cv2.COLORMAP_JET), (UNIT_W, UNIT_H))\n",
    "conf_0_scale_1 = cv2.resize(cv2.applyColorMap((255 *  conf0[1] / ( conf0[1].max() + 1e-10)).astype(np.uint8), cv2.COLORMAP_JET), (UNIT_W, UNIT_H))\n",
    "conf_0_scale_2 = cv2.resize(cv2.applyColorMap((255 *  conf0[2] / ( conf0[2].max() + 1e-10)).astype(np.uint8), cv2.COLORMAP_JET), (UNIT_W, UNIT_H))\n",
    "conf_1_scale_0 = cv2.resize(cv2.applyColorMap((255 *  conf1[0] / ( conf0[0].max() + 1e-10)).astype(np.uint8), cv2.COLORMAP_JET), (UNIT_W, UNIT_H))\n",
    "conf_1_scale_1 = cv2.resize(cv2.applyColorMap((255 *  conf1[1] / ( conf0[1].max() + 1e-10)).astype(np.uint8), cv2.COLORMAP_JET), (UNIT_W, UNIT_H))\n",
    "conf_1_scale_2 = cv2.resize(cv2.applyColorMap((255 *  conf1[2] / ( conf0[2].max() + 1e-10)).astype(np.uint8), cv2.COLORMAP_JET), (UNIT_W, UNIT_H))\n",
    "\n",
    "\n",
    "def drawText(array, text='', w=0.6, color=(0,255,255)):\n",
    "    return cv2.putText(array, text, (int(UNIT_W*w), int(UNIT_H*0.9)),cv2.FONT_HERSHEY_SIMPLEX,4,color,15)\n",
    "\n",
    "\n",
    "pil_input0 = np.array(pil_input0)\n",
    "pil_input1 = np.array(pil_input1)\n",
    "pil_input0 = drawPoint(pil_input0, target[0][\"points\"])\n",
    "pil_input1 = drawPoint(pil_input1, target[1][\"points\"])\n",
    "\n",
    "pil_input0= drawText( pil_input0, 'GT: {:.2f}'.format(gt_count_0),color=(255,255,0))\n",
    "pil_input1= drawText( pil_input1, 'GT: {:.2f}'.format(gt_count_1), color=(255,255,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(den_scales_1_map)):\n",
    "    den_scales_1_map[i]= drawText( den_scales_1_map[i], 'Pre: {:.2f}'.format(count_0_scale[i]))\n",
    "    gt_den_scales_1_map[i]= drawText( gt_den_scales_1_map[i], 'GT: {:.2f}'.format(gt_count_0))\n",
    "    den_scales_2_map[i]= drawText( den_scales_2_map[i], 'Pre: {:.2f}'.format(count_1_scale[i]))\n",
    "    gt_den_scales_2_map[i]= drawText( gt_den_scales_2_map[i], 'GT: {:.2f}'.format(gt_count_1))\n",
    "\n",
    "\n",
    "    den_scales_1_map[i]= Image.fromarray(cv2.cvtColor(den_scales_1_map[i], cv2.COLOR_BGR2RGB))\n",
    "    gt_den_scales_1_map[i]= Image.fromarray(cv2.cvtColor(gt_den_scales_1_map[i], cv2.COLOR_BGR2RGB))\n",
    "    den_scales_2_map[i]= Image.fromarray(cv2.cvtColor(den_scales_2_map[i], cv2.COLOR_BGR2RGB))\n",
    "    gt_den_scales_2_map[i]= Image.fromarray(cv2.cvtColor(gt_den_scales_2_map[i], cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    gt_mask_out_scales_1_map[i]= drawText( gt_mask_out_scales_1_map[i], 'GT Out: {:.2f}'.format(gt_out),0.53)\n",
    "    gt_mask_in_scales_1_map[i]= drawText( gt_mask_in_scales_1_map[i], 'GT In: {:.2f}'.format(gt_in),0.58)\n",
    "    mask_out_scales_1_map[i]= drawText( mask_out_scales_1_map[i], 'Pre Out: {:.2f}'.format(count_out),0.5)\n",
    "    mask_in_scales_1_map[i]= drawText( mask_in_scales_1_map[i], 'Pre In: {:.2f}'.format(count_in),0.55)\n",
    "\n",
    "    mask_out_scales_1_map[i]= Image.fromarray(cv2.cvtColor(mask_out_scales_1_map[i], cv2.COLOR_BGR2RGB))\n",
    "    mask_in_scales_1_map[i]= Image.fromarray(cv2.cvtColor(mask_in_scales_1_map[i], cv2.COLOR_BGR2RGB))\n",
    "    gt_mask_out_scales_1_map[i]= Image.fromarray(cv2.cvtColor(gt_mask_out_scales_1_map[i], cv2.COLOR_BGR2RGB))\n",
    "    gt_mask_in_scales_1_map[i]= Image.fromarray(cv2.cvtColor(gt_mask_in_scales_1_map[i], cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pil_input0 = Image.fromarray(cv2.cvtColor(pil_input0, cv2.COLOR_BGR2RGB))\n",
    "# pil_input1 = Image.fromarray(cv2.cvtColor(pil_input1, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "pil_input0 = Image.fromarray(pil_input0)\n",
    "pil_input1 = Image.fromarray(pil_input1)\n",
    "\n",
    "den0_map= drawText( den0_map, 'Pre: {:.2f}'.format(count_0))\n",
    "den1_map= drawText( den1_map, 'Pre: {:.2f}'.format(count_1))\n",
    "\n",
    "\n",
    "den0_map = Image.fromarray(cv2.cvtColor(den0_map, cv2.COLOR_BGR2RGB))\n",
    "den1_map = Image.fromarray(cv2.cvtColor(den1_map, cv2.COLOR_BGR2RGB))\n",
    "# out_map = Image.fromarray(cv2.cvtColor(out_map, cv2.COLOR_BGR2RGB))\n",
    "# in_map = Image.fromarray(cv2.cvtColor(in_map, cv2.COLOR_BGR2RGB))\n",
    "# gt_out_map = Image.fromarray(cv2.cvtColor(gt_out_map, cv2.COLOR_BGR2RGB))\n",
    "# gt_in_map = Image.fromarray(cv2.cvtColor(gt_in_map, cv2.COLOR_BGR2RGB))\n",
    "conf_map0 = Image.fromarray(cv2.cvtColor(conf_map0, cv2.COLOR_BGR2RGB))\n",
    "conf_map1 = Image.fromarray(cv2.cvtColor(conf_map1, cv2.COLOR_BGR2RGB))\n",
    "conf_map0_dot = Image.fromarray(cv2.cvtColor(conf_map0_dot, cv2.COLOR_BGR2RGB))\n",
    "conf_map1_dot = Image.fromarray(cv2.cvtColor(conf_map1_dot, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "conf_0_scale_0 = Image.fromarray(cv2.cvtColor(conf_0_scale_0, cv2.COLOR_BGR2RGB))\n",
    "conf_0_scale_1 = Image.fromarray(cv2.cvtColor(conf_0_scale_1, cv2.COLOR_BGR2RGB))\n",
    "conf_0_scale_2 = Image.fromarray(cv2.cvtColor(conf_0_scale_2, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "conf_1_scale_0 = Image.fromarray(cv2.cvtColor(conf_1_scale_0, cv2.COLOR_BGR2RGB))\n",
    "conf_1_scale_1 = Image.fromarray(cv2.cvtColor(conf_1_scale_1, cv2.COLOR_BGR2RGB))\n",
    "conf_1_scale_2 = Image.fromarray(cv2.cvtColor(conf_1_scale_2, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = f\"../visualization/{dataset}/{scene_name}/{img1_frame}_{img2_frame}\"\n",
    "os.makedirs(folder,exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img1_denscale'),exist_ok=True)\n",
    "# os.makedirs(os.path.join(folder,'img1_GTdenscale'),exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img2_denscale'),exist_ok=True)\n",
    "# os.makedirs(os.path.join(folder,'img2_GTdenscale'),exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img1_confscale'),exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img1_confscale'),exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img1_confscale'),exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img2_confscale'),exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img2_confscale'),exist_ok=True)\n",
    "os.makedirs(os.path.join(folder,'img2_confscale'),exist_ok=True)\n",
    "\n",
    "\n",
    "for i in range(len(den_scales_1_map)):\n",
    "    den_scales_1_map[i].save(os.path.join(folder,f\"img1_denscale/{i}.jpg\"))\n",
    "    den_scales_2_map[i].save(os.path.join(folder,f\"img2_denscale/{i}.jpg\"),None)\n",
    "mask_out_scales_1_map[0].save(os.path.join(folder,f\"img1_mask_out.jpg\"),None)\n",
    "mask_in_scales_1_map[0].save(os.path.join(folder,f\"img2_mask_in.jpg\"),None)\n",
    "gt_mask_out_scales_1_map[0].save(os.path.join(folder,f\"img1_mask_out_gt.jpg\"),None)\n",
    "gt_mask_in_scales_1_map[0].save(os.path.join(folder,f\"img2_mask_in_gt.jpg\"),None)\n",
    "\n",
    "gt_den_scales_1_map[0].save(os.path.join(folder,f\"img1_den_gt.jpg\"),None)\n",
    "gt_den_scales_2_map[0].save(os.path.join(folder,f\"img2_den_gt.jpg\"),None)\n",
    "\n",
    "\n",
    "pil_input0.save(os.path.join(folder,f\"img1.jpg\"),None)\n",
    "pil_input1.save(os.path.join(folder,f\"img2.jpg\"),None)\n",
    "den0_map.save(os.path.join(folder,f\"img1_den.jpg\"),None)\n",
    "den1_map.save(os.path.join(folder,f\"img2_den.jpg\"),None)\n",
    "# out_map.save(os.path.join(folder,f\"img1_out_den.jpg\"),None)\n",
    "# in_map.save(os.path.join(folder,f\"img2_in_den.jpg\"),None)\n",
    "# gt_out_map.save(os.path.join(folder,f\"img1_out_den_gt.jpg\"),None)\n",
    "# gt_in_map.save(os.path.join(folder,f\"img2_in_den_gt.jpg\"),None)\n",
    "conf_map0_dot.save(os.path.join(folder,f\"img1_confdot.jpg\"),None)\n",
    "conf_map1_dot.save(os.path.join(folder,f\"img2_confdot.jpg\"),None)\n",
    "conf_0_scale_0.save(os.path.join(folder,f\"img1_confscale/0.jpg\"),None)\n",
    "conf_0_scale_1.save(os.path.join(folder,f\"img1_confscale/1.jpg\"),None)\n",
    "conf_0_scale_2.save(os.path.join(folder,f\"img1_confscale/2.jpg\"),None)\n",
    "conf_1_scale_0.save(os.path.join(folder,f\"img2_confscale/0.jpg\"),None)\n",
    "conf_1_scale_1.save(os.path.join(folder,f\"img2_confscale/1.jpg\"),None)\n",
    "conf_1_scale_2.save(os.path.join(folder,f\"img2_confscale/2.jpg\"),None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
